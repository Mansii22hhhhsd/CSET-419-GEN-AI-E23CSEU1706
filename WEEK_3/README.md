# CSET419 â€“ Introduction to Generative AI  
## Lab 3 â€“ Variational Autoencoder (VAE)

### ğŸ“Œ Objective
In this lab, we implemented a **Variational Autoencoder (VAE)** to understand how generative models learn latent representations and generate new data samples.

The goal was to:
- Learn compressed latent representations
- Generate new images from the learned distribution
- Understand Encoder, Decoder, Latent Space and KL-Divergence

---

### ğŸ§  Theory Overview
A **Variational Autoencoder (VAE)** is a probabilistic generative model.  
Unlike a standard Autoencoder that learns fixed latent vectors, a VAE learns a **distribution (mean and variance)** in the latent space.

It uses:
- Encoder â†’ Learns Î¼ (mean) and log(ÏƒÂ²)
- Reparameterization Trick â†’ Enables backpropagation
- Decoder â†’ Reconstructs images from latent vectors
- Loss Function â†’ Reconstruction Loss + KL Divergence

---

### ğŸ§ª Tasks Performed

#### 1ï¸âƒ£ Dataset Preparation
- Loaded MNIST / Fashion-MNIST dataset  
- Normalized image data  
- Split into training and testing sets  

#### 3ï¸âƒ£ Loss Function
- Reconstruction Loss (Binary Cross Entropy / MSE)  
- KL Divergence Loss  
- Combined both losses  

#### 4ï¸âƒ£ Model Training
- Trained VAE for multiple epochs  
- Monitored training and validation loss  

#### 5ï¸âƒ£ Sample Generation
- Sampled random latent vectors from standard normal distribution  
- Generated new images using Decoder  
